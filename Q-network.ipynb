{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "STATE_DIM  = env.observation_space.shape[0]\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_EPISODES = 1000\n",
    "MAX_T = 250\n",
    "STREAK_TO_END = 50\n",
    "SOLVED_T = 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate():\n",
    "    agent = Agent()\n",
    "    solved = False\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(MAX_T):\n",
    "            # Gym allows for rendering pictures and animations of the environment,\n",
    "            # but due to additional configuration needed for a remote server will not run on the collab\n",
    "            #env.render()\n",
    "\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Execute the action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if done: # terminal state\n",
    "                next_state = None\n",
    "\n",
    "            agent.observe((state, action, reward, next_state))\n",
    "            agent.replay()\n",
    "\n",
    "            # Setting up for the next iteration\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"Episode %d finished after %f time steps\" % (episode, t))\n",
    "                if (t >= SOLVED_T) and evaluate(agent, STREAK_TO_END - 1):\n",
    "                    solved = True\n",
    "                break\n",
    "        if solved:\n",
    "            print(\"Task successfully solved after %d episodes\" % episode)\n",
    "            break\n",
    "        agent.signal_episode_end()\n",
    "\n",
    "def evaluate(agent, streak):\n",
    "    for _ in range(streak):\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(MAX_T):\n",
    "            action = agent.act(state, explore=False)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                if (t < SOLVED_T):\n",
    "                    return False\n",
    "                break\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEMORY_CAPACITY = 100000\n",
    "DISCOUNT_FACTOR = 0.95\n",
    "MAX_EXPLORATION_RATE = 1.0\n",
    "MIN_EXPLORATION_RATE = 0.01\n",
    "DECAY_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.explore_rate = MAX_EXPLORATION_RATE\n",
    "        self.brain = Brain()\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        self.steps = 0\n",
    "\n",
    "    def act(self, s, explore=True):\n",
    "        if explore and random.random() < self.explore_rate:\n",
    "            return random.randint(0, NUM_ACTIONS - 1)\n",
    "        else:\n",
    "            return np.argmax(self.brain.predict([s])[0])\n",
    "\n",
    "    def observe(self, sample):\n",
    "        self.steps += 1\n",
    "        self.memory.add(sample)\n",
    "\n",
    "        # Reduces exploration rate linearly\n",
    "        self.explore_rate = MIN_EXPLORATION_RATE + (MAX_EXPLORATION_RATE - MIN_EXPLORATION_RATE) \\\n",
    "                            * math.exp(-DECAY_RATE * self.steps)\n",
    "\n",
    "    def replay(self):\n",
    "        batch = self.memory.get_random_samples(self.brain.BATCH_SIZE)\n",
    "        batchLen = len(batch)\n",
    "\n",
    "        states = np.array([sample[0] for sample in batch], dtype=np.float32)\n",
    "        no_state = np.zeros(STATE_DIM)\n",
    "        resultant_states = np.array([(no_state if sample[3] is None else sample[3]) for sample in batch], dtype=np.float32)\n",
    "\n",
    "        q_values_batch = self.brain.target_predict(states)\n",
    "        future_q_values_batch = self.brain.target_predict(resultant_states)\n",
    "        # q_values_batch = self.brain.predict(states)\n",
    "        # future_q_values_batch = self.brain.predict(resultant_states)\n",
    "\n",
    "        x = np.zeros((batchLen, STATE_DIM)).astype(np.float32)\n",
    "        y = np.zeros((batchLen, NUM_ACTIONS)).astype(np.float32)\n",
    "\n",
    "        for i in range(batchLen):\n",
    "            state, action, reward, resultant_state = batch[i]\n",
    "\n",
    "            q_values = q_values_batch[i]\n",
    "            if resultant_state is None:\n",
    "                q_values[action] = reward\n",
    "            else:\n",
    "                q_values[action] = reward + DISCOUNT_FACTOR * np.amax(future_q_values_batch[i])\n",
    "\n",
    "            x[i] = state\n",
    "            y[i] = q_values\n",
    "\n",
    "        self.brain.train(x, y)\n",
    "        if not self.steps%50:\n",
    "            self.brain.transfer_variables()\n",
    "\n",
    "    def signal_episode_end(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 50\n",
    "        \n",
    "        self.__observation = tf.placeholder(tf.float32, [None, STATE_DIM])\n",
    "        self.__q_target = tf.placeholder(tf.float32, [None, NUM_ACTIONS])\n",
    "\n",
    "        self.model = self.create_multi_layer_neural_network(self.__observation, NUM_ACTIONS, 3, \"train\")\n",
    "        self.target_net = self.create_multi_layer_neural_network(self.__observation, NUM_ACTIONS, 3, \"target_net\")\n",
    "        with tf.variable_scope(\"None\"):\n",
    "            self.learning_rate = 0.05\n",
    "            self.loss = tf.reduce_mean(tf.square(self.model-self.__q_target))\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=self.learning_rate/self.BATCH_SIZE).minimize(self.loss)\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.transfer_variables()\n",
    "\n",
    "    def train(self, x, y):\n",
    "        self.session.run([self.trainer], {self.__observation: x, self.__q_target: y})\n",
    "\n",
    "    def predict(self, s):\n",
    "        return self.session.run(self.model, {self.__observation: s})\n",
    "\n",
    "    def target_predict(self, s):\n",
    "        return self.session.run(self.target_net, {self.__observation: s})\n",
    "\n",
    "    def transfer_variables(self):\n",
    "        col1 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='train')\n",
    "        col2 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n",
    "        assign_op = []\n",
    "        assert(len(col1)==len(col2))\n",
    "        assert([v.name.split(\"/\")[-1] for v in col1]==[v.name.split(\"/\")[-1] for v in col2])\n",
    "        assign_op = [v2.assign(v1) for v1, v2 in zip(col1, col2)]\n",
    "        self.session.run(assign_op)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_multi_layer_neural_network(input_vars, out_dims, num_hidden_layers, namescope):\n",
    "        model = None\n",
    "        with tf.variable_scope(namescope):\n",
    "            input_dims = input_vars.shape[1].value\n",
    "            num_hidden_neurons = 50\n",
    "            last_layer = input_vars\n",
    "\n",
    "            for k in range(num_hidden_layers):\n",
    "                last_layer = tf.contrib.layers.fully_connected(last_layer, num_hidden_neurons,\\\n",
    "                            activation_fn=tf.nn.relu, biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "            model = tf.contrib.layers.fully_connected(last_layer, out_dims,\\\n",
    "                            activation_fn=None, biases_initializer=tf.zeros_initializer())\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def create_single_layer_neural_network(input_vars, out_dims):\n",
    "        return Brain.create_multi_layer_neural_network(input_vars, out_dims, 1)\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, capacity):\n",
    "        self.examplers = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.examplers.append(sample)\n",
    "\n",
    "    def get_random_samples(self, num_samples):\n",
    "        num_samples = min(num_samples, len(self.examplers))\n",
    "        return random.sample(tuple(self.examplers), num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 16.000000 time steps\n",
      "Episode 1 finished after 35.000000 time steps\n",
      "Episode 2 finished after 13.000000 time steps\n",
      "Episode 3 finished after 21.000000 time steps\n",
      "Episode 4 finished after 10.000000 time steps\n",
      "Episode 5 finished after 68.000000 time steps\n",
      "Episode 6 finished after 35.000000 time steps\n",
      "Episode 7 finished after 18.000000 time steps\n",
      "Episode 8 finished after 25.000000 time steps\n",
      "Episode 9 finished after 21.000000 time steps\n",
      "Episode 10 finished after 14.000000 time steps\n",
      "Episode 11 finished after 22.000000 time steps\n",
      "Episode 12 finished after 17.000000 time steps\n",
      "Episode 13 finished after 8.000000 time steps\n",
      "Episode 14 finished after 18.000000 time steps\n",
      "Episode 15 finished after 9.000000 time steps\n",
      "Episode 16 finished after 14.000000 time steps\n",
      "Episode 17 finished after 13.000000 time steps\n",
      "Episode 18 finished after 12.000000 time steps\n",
      "Episode 19 finished after 12.000000 time steps\n",
      "Episode 20 finished after 14.000000 time steps\n",
      "Episode 21 finished after 22.000000 time steps\n",
      "Episode 22 finished after 16.000000 time steps\n",
      "Episode 23 finished after 17.000000 time steps\n",
      "Episode 24 finished after 35.000000 time steps\n",
      "Episode 25 finished after 11.000000 time steps\n",
      "Episode 26 finished after 20.000000 time steps\n",
      "Episode 27 finished after 15.000000 time steps\n",
      "Episode 28 finished after 11.000000 time steps\n",
      "Episode 29 finished after 14.000000 time steps\n",
      "Episode 30 finished after 12.000000 time steps\n",
      "Episode 31 finished after 10.000000 time steps\n",
      "Episode 32 finished after 9.000000 time steps\n",
      "Episode 33 finished after 21.000000 time steps\n",
      "Episode 34 finished after 15.000000 time steps\n",
      "Episode 35 finished after 18.000000 time steps\n",
      "Episode 36 finished after 22.000000 time steps\n",
      "Episode 37 finished after 19.000000 time steps\n",
      "Episode 38 finished after 34.000000 time steps\n",
      "Episode 39 finished after 65.000000 time steps\n",
      "Episode 40 finished after 199.000000 time steps\n",
      "Episode 41 finished after 199.000000 time steps\n",
      "Task successfully solved after 41 episodes\n"
     ]
    }
   ],
   "source": [
    "simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque\n",
    "\n",
    "NUM_BINS = 8\n",
    "MIN_VALUE = -10\n",
    "MAX_VALUE = 10\n",
    "\n",
    "def bin_mids():\n",
    "    return np.array([MIN_VALUE+(i+0.5)*(MAX_VALUE-MIN_VALUE)/NUM_BINS for i in range(NUM_BINS)])\n",
    "\n",
    "def values_to_bin_probs(values, probs):\n",
    "    new_probs = []\n",
    "    right_end = MIN_VALUE\n",
    "    for i in range(NUM_BINS):\n",
    "        left_end = right_end\n",
    "        right_end = left_end + (MAX_VALUE-MIN_VALUE)/NUM_BINS\n",
    "        prob = sum(p for p, v in zip(probs, values) if right_end > v >= left_end)\n",
    "        new_probs.append(prob)\n",
    "    return new_probs\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.explore_rate = MAX_EXPLORATION_RATE\n",
    "        self.brain = Brain()\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        self.steps = 0\n",
    "\n",
    "    def act(self, s, explore=True):\n",
    "        if explore and random.random() < self.explore_rate:\n",
    "            return random.randint(0, NUM_ACTIONS - 1)\n",
    "        else:\n",
    "            probs = self.brain.predict([s])[0]\n",
    "            return np.argmax(np.sum(probs*bin_mids(), axis=1))\n",
    "\n",
    "    def observe(self, sample):\n",
    "        self.steps += 1\n",
    "        self.memory.add(sample)\n",
    "\n",
    "        # Reduces exploration rate linearly\n",
    "        self.explore_rate = MIN_EXPLORATION_RATE + (MAX_EXPLORATION_RATE - MIN_EXPLORATION_RATE) \\\n",
    "                            * math.exp(-DECAY_RATE * self.steps)\n",
    "\n",
    "    def replay(self):\n",
    "        batch = self.memory.get_random_samples(self.brain.BATCH_SIZE)\n",
    "        batchLen = len(batch)\n",
    "\n",
    "        states = np.array([sample[0] for sample in batch], dtype=np.float32)\n",
    "        no_state = np.zeros(STATE_DIM)\n",
    "        resultant_states = np.array([(no_state if sample[3] is None else sample[3]) for sample in batch], dtype=np.float32)\n",
    "\n",
    "        # q_values_batch = self.brain.target_predict(states)\n",
    "        # future_q_values_batch = self.brain.target_predict(resultant_states)\n",
    "        q_values_batch = self.brain.predict(states)\n",
    "        future_q_values_batch = self.brain.predict(resultant_states)\n",
    "\n",
    "        x = np.zeros((batchLen, STATE_DIM)).astype(np.float32)\n",
    "        y = np.zeros((batchLen, NUM_ACTIONS, NUM_BINS)).astype(np.float32)\n",
    "\n",
    "        for i in range(batchLen):\n",
    "            state, action, reward, resultant_state = batch[i]\n",
    "\n",
    "            q_values = q_values_batch[i]\n",
    "            if resultant_state is None:\n",
    "                q_values[action] = reward\n",
    "            else:\n",
    "                next_action = self.act(resultant_state)\n",
    "                next_probs = future_q_values_batch[i][next_action]\n",
    "                next_vals = reward + DISCOUNT_FACTOR * bin_mids()\n",
    "                q_values[action] = values_to_bin_probs(next_vals, next_probs)\n",
    "\n",
    "            x[i] = state\n",
    "            y[i] = q_values\n",
    "\n",
    "        self.brain.train(x, y)\n",
    "        if not self.steps%50:\n",
    "            self.brain.transfer_variables()\n",
    "\n",
    "    def signal_episode_end(self):\n",
    "        pass\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 50\n",
    "        \n",
    "        self.__observation = tf.placeholder(tf.float32, [None, STATE_DIM])\n",
    "        self.__q_target = tf.placeholder(tf.float32, [None, NUM_ACTIONS, NUM_BINS])\n",
    "\n",
    "        self.model = self.create_multi_layer_neural_network(self.__observation, NUM_ACTIONS*NUM_BINS, 3, \"train\")\n",
    "        self.target_net = self.create_multi_layer_neural_network(self.__observation, NUM_ACTIONS*NUM_BINS, 3, \"target_net\")\n",
    "        with tf.variable_scope(\"None\"):\n",
    "            self.learning_rate = 0.05\n",
    "            self.loss = tf.reduce_mean(tf.square(self.model-self.__q_target))\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=self.learning_rate/self.BATCH_SIZE).minimize(self.loss)\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.transfer_variables()\n",
    "\n",
    "    def train(self, x, y):\n",
    "        self.session.run([self.trainer], {self.__observation: x, self.__q_target: y})\n",
    "\n",
    "    def predict(self, s):\n",
    "        return self.session.run(self.model, {self.__observation: s})\n",
    "\n",
    "    def target_predict(self, s):\n",
    "        return self.session.run(self.target_net, {self.__observation: s})\n",
    "\n",
    "    def transfer_variables(self):\n",
    "        col1 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='train')\n",
    "        col2 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n",
    "        assign_op = []\n",
    "        assert(len(col1)==len(col2))\n",
    "        assert([v.name.split(\"/\")[-1] for v in col1]==[v.name.split(\"/\")[-1] for v in col2])\n",
    "        assign_op = [v2.assign(v1) for v1, v2 in zip(col1, col2)]\n",
    "        self.session.run(assign_op)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_multi_layer_neural_network(input_vars, out_dims, num_hidden_layers, namescope):\n",
    "        model = None\n",
    "        with tf.variable_scope(namescope):\n",
    "            input_dims = input_vars.shape[1].value\n",
    "            num_hidden_neurons = 50\n",
    "            last_layer = input_vars\n",
    "\n",
    "            for k in range(num_hidden_layers):\n",
    "                last_layer = tf.contrib.layers.fully_connected(last_layer, num_hidden_neurons,\\\n",
    "                            activation_fn=tf.nn.relu, biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "            last_layer = tf.contrib.layers.fully_connected(last_layer, out_dims,\\\n",
    "                            activation_fn=None, biases_initializer=tf.zeros_initializer())\n",
    "            divided = tf.split(last_layer, NUM_ACTIONS, axis=1)\n",
    "            model = tf.reshape(tf.convert_to_tensor([tf.nn.softmax(part) for part in divided]), (-1, NUM_ACTIONS, NUM_BINS))\n",
    "            print(model.shape)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def create_single_layer_neural_network(input_vars, out_dims):\n",
    "        return Brain.create_multi_layer_neural_network(input_vars, out_dims, 1)\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, capacity):\n",
    "        self.examplers = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.examplers.append(sample)\n",
    "\n",
    "    def get_random_samples(self, num_samples):\n",
    "        num_samples = min(num_samples, len(self.examplers))\n",
    "        return random.sample(tuple(self.examplers), num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
